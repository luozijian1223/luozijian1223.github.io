[{"content":"书本链接：定投 —— 大佬的自我修养\n小马过河，独立思考 “都涨这么多了，还会再涨吗？！天下哪儿有不停地涨的东西？！” 然后，很冷静地交出了筹码。\n另外一个相关的病症是这样的，他们会说，“这个标的咋这么贵？” 比如中国股票市场上的茅台股票，比如已经流通十年之后的比特币…… 绝大多数人只是因为茅台股票的价格或者比特币的价格绝对值看起来很高，就退怯了，转而用尽他们的聪明才智去寻找 “下一支十倍股”、“下一个涨停股” 或者 “下一个百倍币”—— 绝大多数都没有成功。为什么呢？因为投资不是看价格的，是看增长潜力的。很多人误以为基数小就更容易增长，这是完全没有依据的。\n另外一个病症是之前提过的 “频繁更换车道”。这种人就好像在路上堵车的时候不断并线换道的人一样。他买了一个标的，但， 没多久就会发现别的标的涨得比自己手中的这个标的更多…… 然后就忍不住了，并线换道，把自己手中的标的卖出去，买入另外一个标的。可是，没多久，他就会发现新标的到了他手中之后，就涨得不像它原来那么猛了，甚至反倒开始跌了…… 自认倒霉的他可能还会换回去，直到承认自己的确不适合干这行为止。\n在任何一个系统内，都有既得利益者和非既得利益者；前者是极少数，后者是绝大多数。于是，教育的目标很清楚，既得利益者会利用它使其对自己更为有利 —— 不用我说，你都知道他们会怎么做了！非既得利益者在紧张的对峙过程中很快就会发现势不均力不敌，于是绝大多数转而服务于既得利益者；少数依然反抗的非既得利益者最终会被消灭；如果某一次非既得利益者的反抗竟然成功了，那么他们会瞬间变成既得利益者，开启下一个同样是服务于既得利益者的 “新的” 系统…… 这是无差别的事实。\n","permalink":"https://luozijian1223.github.io/posts/read/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E5%AE%9A%E6%8A%95--%E5%A4%A7%E4%BD%AC%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/","summary":"书本链接：定投 —— 大佬的自我修养 小马过河，独立思考 “都涨这么多了，还会再涨吗？！天下哪儿有不停地涨的东西？！” 然后，很冷静地交出了筹码。 另外","title":"【读书笔记】定投 —— 大佬的自我修养"},{"content":"阿加莎·克里斯蒂的《无人生还》像一记闷棍，狠狠敲打在我的心上。合上书页，那座与世隔绝的士兵岛，那十个形态各异的小瓷人，以及那首令人毛骨悚然的童谣，依旧在脑海中挥之不去。这不仅仅是一部推理小说，更是一面映照人性幽微之处的镜子，一场关于罪恶、审判与救赎的深刻探讨。\n我最大的感触是：“正义”的迷局：谁有资格裁决？\n小说最引人深思的，莫过于“法官”沃格雷夫精心设计的这场“正义审判”。他以一种近乎上帝的视角，将十个逃脱了法律制裁的人聚集到孤岛，依据他们各自的罪行，逐一处决。他自诩正义的化身，认为自己是在替天行道。\n然而，这种“正义”真的站得住脚吗？沃格雷夫的审判，与其说是正义的伸张，不如说是一种私刑的极端化。他既是控方，又是辩方，更是法官和执行者，完全绕开了现代司法体系的程序正义。他所依据的“罪行”，有些是板上钉钉的谋杀，有些则是间接的过失，甚至还有道德层面的谴责（比如麦克阿瑟将军让下属送死）。这种将不同性质的“罪”混为一谈，并一律处以极刑的做法，本身就是一种不公正。\n更可怕的是，沃格雷夫的“正义”背后，隐藏着他个人的扭曲心理。他享受着掌控他人命运的快感，陶醉于自己“惩恶扬善”的幻觉中。他将自己凌驾于法律之上，将个人意志等同于天意，这无疑是一种极度的自负和疯狂。\n联系到现实，我们不难发现，类似的“正义迷局”并不少见，譬如网络暴力。当某个事件曝光，引发公众关注时，网民们往往会迅速站队，对当事人进行道德审判，甚至人肉搜索、肆意谩骂。这种“键盘侠”式的正义，往往缺乏理性思考，容易被情绪裹挟，甚至演变成一种网络暴力，对当事人造成二次伤害。\n《无人生还》是一部值得反复品味的作品。它不仅仅是一部引人入胜的推理小说，更是一部关于人性、罪恶和救赎的深刻寓言。它让我们思考，什么是真正的正义？谁有资格进行审判？在极端的环境下，人性会走向何方？这些问题，没有标准答案，但每一次阅读，都会带给我们新的启示。它像一面镜子，逼着我们直视人性的幽暗，也提醒我们，即使在最绝望的时刻，也不要放弃对光明和希望的追寻。\n","permalink":"https://luozijian1223.github.io/posts/read/%E6%97%A0%E4%BA%BA%E7%94%9F%E8%BF%98/","summary":"阿加莎·克里斯蒂的《无人生还》像一记闷棍，狠狠敲打在我的心上。合上书页，那座与世隔绝的士兵岛，那十个形态各异的小瓷人，以及那首令人毛骨悚然的","title":"《无人生还》：孤岛上的罪恶、审判与人性挣扎"},{"content":"![[Pasted image 20250218162344.png]]\n填充（padding）和掩码（masking）机制 在解码器（decoder）中使用掩码（mask）的目的是确保当前位置的输出仅仅依赖于其之前的位置，而不是未来的位置。\n掩码（mask）的作用 1. 因果关系（Causality） 在序列生成中，每个输出仅仅应当依赖于之前的输出。例如，生成句子的第四个单词时，我们只能考虑前三个单词，而不能考虑第五个单词。加入掩码确保模型在训练时保持这种因果关系。\n2. 训练与推理的一致性 在推理（即实际使用）时，我们通常采用自回归方式生成序列：一次生成一个标记，然后将其作为新输入传递回模型。使用掩码确保模型在训练和推理时的行为是一致的。\n3. 避免信息泄露 如果不使用掩码，模型可能会在训练期间“窥视”未来的标记，并过度依赖这些信息。这会导致在推理时性能下降，因为在实际应用中这些未来的标记是不可用的。\n在实际操作中，掩码通常是一个上三角形状的矩阵，其中上三角（包括对角线）的部分设为0，其余部分设为负无穷（或非常大的负数）。在应用 softmax 函数计算注意力权重之前，这个掩码会被加到注意力分数上，这样上三角部分的分数在 softmax 后基本上就会变成0，从而实现了掩蔽效果。\n举例 “我喜欢猫猫” “我喜欢打羽毛球” 步骤 1：Tokenization 我们将每个句子分词，得到以下token序列：\n1. “我” “喜” “欢” “猫” “猫” 2. “我” “喜” “欢” “打” “羽” “毛” “球” 步骤 2：Padding 由于训练集中最长的句子长度为10个tokens，我们需要对这两个句子进行填充，使它们的长度都达到10个tokens。通常使用一个特殊的填充token（例如\u0026quot;\u0026quot;）来实现这一点：\n1. “我” “喜” “欢” “猫” “猫” “” “” “” “” “” 2. “我” “喜” “欢” “打” “羽” “毛” “球” “” “” “” 步骤 3：Masking 现在，模型需要知道哪些是真正的token，哪些是填充的token。这是通过创建一个掩码矩阵来实现的，掩码矩阵与输入序列的长度相同。在这个掩码矩阵中，真实token的位置用0表示，填充token的位置用1表示：\n1. 掩码：[0, 0, 0, 0, 0, 1, 1, 1, 1, 1] 2. 掩码：[0, 0, 0, 0, 0, 0, 0, 1, 1, 1] 步骤 4：应用掩码 在模型的注意力机制中，特别是在计算softmax之前，我们会将这些掩码应用到模型中。具体来说，我们会在掩码为1的位置上加上一个非常大的负数（例如负无穷），这样在经过softmax操作后，这些位置的值将接近0，从而不会对全局概率预测产生影响。\n通过这种方式，模型在计算注意力权重时，会忽略填充token，确保只关注实际的内容token。这样，模型就能够正确地学习序列数据，而不会被填充token所干扰。\nMask-Multi-Head-Attention Mask 的目的是防止 Decoder “seeing the future”，就像防止考生偷看考试答案一样。\n[1](Pasted image 20250218182106.png)\nLinear 和 Softmax 来生成输出概率 在解码器的最终阶段，其输出将通过一个线性层进行转换，该层的作用是将解码器的输出映射到与词汇表大小相匹配的维度。随后，应用softmax函数将这些数值转换成概率分布，其中每个词汇对应一个概率值。在这一过程中，概率最高的词汇被选为预测的下一个词。\n训练过程 模型没有收敛得很好时，Decoder预测产生的词很可能不是我们想要的。这个时候如果再把错误的数据再输给Decoder，就会越跑越偏。这个时候怎么办？\n(1）在训练过程中可以使用 “teacher forcing”。因为我们知道应该预测的word是什么，那么可以给Decoder喂一个正确的结果作为输入。\n(2）除了选择最高概率的词 (greedy search)，还可以选择是比如 “Beam Search”，保留top-K个预测的word。 Beam Search 方法不再是只得到一个输出放到下一步去训练了，我们可以设定一个值，拿多个值放到下一步去训练，这条路径的概率等于每一步输出的概率的乘积。\n训练和损失函数 在训练过程中，我们使用损失函数（例如交叉熵损失）将生成的输出概率分布与目标序列进行比较。概率分布给出了每个单词出现在该位置的概率。\nDecoder在不同阶段的信息传递机制 训练阶段 自由运行（Free Running）模式 在训练阶段，解码器使用上一个时间单元的预测值作为下一个时间单元的输入。这种方法的缺点是，如果模型在某一步预测错误，那么这个错误会随着时间单元的推进而累积，导致预测结果越来越偏离正确轨迹，从而增加了训练的难度。 教师强制（Teacher Forcing）模式 Teacher Forcing模式在训练时使用真实的标签（label）作为输入，而不是模型的预测值。这样，即使模型在前一个时间单元预测错误，它仍然可以在下一个时间单元接收到正确的信息，从而更容易地学习和收敛。但是，这种方法的局限性在于，它在训练和预测阶段存在不一致性，因为在实际预测时，我们无法获得真实的标签。 结合这两种方法的优点，在训练阶段采用了一种称为计划采样（scheduled sampling）的策略。\n核心思想：在训练过程中动态地结合真实标签和模型的预测。 具体做法：设定一个概率值p，以p的概率，使用上一个时间单元的预测值作为输入，以1-p的概率，使用真实的标签作为输入。这样，解码器在训练时既能得到真实标签的适当指导，又能逐渐学会依赖自己的预测，从而提高了模型在预测阶段的稳健性和一致性。\n预测阶段 穷举法 贪心搜索 束搜索 在预测阶段，解码器直接将前一个时间单元的输出（即预测值）作为输入传递给下一个时间单元。 Decoder的输入选择：\n- 穷举法 softmax对所有候选词预测概率，然后使用所有的排列组合找出输出条件概率最大的序列，作为decoder的输出，可以保证全局最优，但是计算复杂度非常高\n- 贪心搜索： 使用softmax对所有候选词预测概率，选概率最大的词作为当前单元的输出，**保证每一步输出最优解，但满足局部最优策略期望未必产生全局最优解\n- 束搜索（Beam search）： 在贪心搜索上做了个优化，设置一个束宽beam size，然后softmax预测出来的候选词概率选取概率最大的beam size个词输入下一个单元，比如下面这个例子，beam size设置为2，每个时间单元会从候选词中选择概率值最大的两个词，然后输入到下一个时间单元，在计算条件概率，然后再去最大的前两个，以此类推\n模型的训练与评估 训练Transformer模型涉及优化其参数以最小化损失函数，通常使用梯度下降和反向传播。一旦训练完成，就会使用各种指标评估模型的性能，以评估其解决目标任务的有效性。\n梯度下降 (Gradient Descent)是一种迭代优化算法，用于最小化损失函数。 它通过沿着负梯度方向逐步更新参数，寻找最优解。\n反向传播 (Backpropagation)利用 链式法则，将梯度从输出层反向传播到输入层，计算出所有参数的梯度。\n训练神经网络的完整流程\n随机初始化参数 \u0026ndash;\u0026gt; 前向传播 \u0026ndash;\u0026gt; 反向传播 \u0026ndash;\u0026gt; 参数更新 \u0026ndash;\u0026gt; 重复步骤 2-4，直到模型收敛或达到预设的训练轮数\n训练过程\n(1) 梯度下降和反向传播：\n在训练期间，将输入序列输入模型，并生成输出序列。 将模型的预测与地面真相进行比较，涉及使用损失函数（例如交叉熵损失）来衡量预测值与实际值之间的差异。 梯度下降用于更新模型的参数，使损失最小化的方向。 优化器根据这些梯度调整参数，迭代更新它们以提高模型性能。 (2) 学习率调度：\n可以应用学习率调度技术来动态调整训练期间的学习率。 常见策略包括热身计划，其中学习率从低开始逐渐增加，以及衰减计划，其中学习率随时间降低。 梯度下降和反向传播 梯度下降是一种优化算法，用于最小化一个损失函数 (Loss Function)。 损失函数衡量了模型预测结果与真实结果之间的差距。 梯度下降是一种 迭代优化算法，用于寻找 损失函数的最小值。\n一些名词：\n损失函数 (Loss Function): 用 𝐽(𝜃)表示，其中 𝜃 代表模型的参数 (例如神经网络的权重和偏置)。 梯度 (Gradient): 用 ∇𝐽(𝜃) 表示，它是一个向量，指向 损失函数 𝐽(𝜃) 在参数空间中变化最快的方向 (也就是 最陡峭的上升方向)。 负梯度 (-∇J(θ)): 负梯度方向 就是 损失函数 𝐽(𝜃) 下降最快的方向，也就是我们要 \u0026ldquo;下山\u0026rdquo; 的方向。 学习率 (Learning Rate, α): 一个 超参数，控制每次迭代 参数更新的步长。 学习率过大可能导致震荡或无法收敛，学习率过小则收敛速度过慢。 参数更新规则: 在每次迭代中，我们根据 负梯度方向 和 学习率 更新参数 𝜃\nθ_(new) = θ_(old) - α * ∇J(θ_(old)) 梯度下降过程：\n初始化：随机化模型参数𝜃。 计算梯计损失函数 𝐽(𝜃) 关于参数𝜃的梯度 ∇𝐽(𝜃) 。这通常需要用到 反向传播算法 更新参数: 根据学习率α 和梯度 ∇𝐽(𝜃)，使用更新规则𝜃=𝜃−𝛼∗∇𝐽(𝜃) 更新参数。 判断是否满足停止条件: 检查是否满足停止条件 (例如梯度是否足够小，损失函数值是否收敛)。 如果满足，停止迭代，当前参数 𝜃 即为最优解 (或近似最优解)。 如果不满足，返回步骤 2，继续迭代。 模型的评估指标 BLEU（Bilingual Evaluation Understudy）：适用于机器翻译和文本生成任务，衡量生成文本与参考文本的相似度。 ROUGE（Recall-Oriented Understudy for Gisting Evaluation）：常用于文本摘要，评估生成摘要与参考摘要的重叠部分。 METEOR（Metric for Evaluation of Translation with Explicit ORdering of Renderings）：结合精确率和召回率，用于评估翻译质量。 Perplexity 困惑度：用于语言模型，衡量模型预测测试数据的能力，数值越低越好。 ","permalink":"https://luozijian1223.github.io/posts/tech/decoder/","summary":"![[Pasted image 20250218162344.png]] 填充（padding）和掩码（masking）机制 在解码器（decoder）中使用掩码（mask）的目的是确保当前位置的输出仅仅依赖于","title":"Decoder"},{"content":"浏览器工作原理 1、User Interface 用户界面，我们所看到的浏览器\n2、Browser engine 浏览器引擎，用来查询和操作渲染引擎\n3、Rendering engine 用来显示请求的内容，负责解析HTML、CSS\n4、Networking 网络，负责发送网络请求\n5、JavaScript Interpreter(解析者) JavaScript解析器，负责执行JavaScript的代码\n6、UI Backend UI后端，用来绘制类似组合框和弹出窗口\n7、Data Persistence(持久化) 数据持久化，数据存储 cookie、HTML5中的sessionStorage\n","permalink":"https://luozijian1223.github.io/posts/tech/%E6%B5%8F%E8%A7%88%E5%99%A8/","summary":"浏览器工作原理 1、User Interface 用户界面，我们所看到的浏览器 2、Browser engine 浏览器引擎，用来查询和操作渲染引擎 3、Rendering engine 用来显","title":"浏览器"},{"content":"0. Prompt 我们每一次访问大模型的输入为一个 Prompt，而大模型给我们的返回结果则被称为 Completion。\nPrompt 是输入指令、控制模型行为的主要方式，作用是输入文本，用于引导 LLM 完成特定任务或生成特定类型的文本。Prompt 至关重要，直接影响模型输出质量。\n1. Temperature 控制 Temprature 参数可以控制 LLM 生成结果的随机性与创造性。\n因为LLM 生成是具有随机性的，在模型的顶层通过选取不同预测概率的预测结果来生成最后的结果。\nTemprature 的取值\n当取值较低接近0时，预测的随机性会较低，产生更保守、可预测的文本，不太可能生成意想不到或不寻常的词。 当取值较高接近 1 时，预测的随机性会较高，所有词被选择的可能性更大，会产生更有创意、多样化的文本，更有可能生成不寻常或意想不到的词。 PS：不同厂家的 Temperature 范围有所不同，Google AI Studio 里的Temperature的范围是0-2。\nTemperature 高的时候，文本更具有创造性。\n这里更加明显，Temperature低的时候，生成的选题相比Temperature高的更加保守。Temperature高的时候，给出的选题更加“放飞”和深入。\n测试地址\n2. System Prompt \u0026amp; User Prompt System Prompt会在整个会话过程中持久地影响模型的回复，且相比于普通 Prompt 具有更高的重要性。System Prompt 常用来对模型进行一些初始化设定。\nUser Prompt，就是指平时的 Prompt，即需要模型做出回复的输入的Prompt。\n","permalink":"https://luozijian1223.github.io/posts/tech/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","summary":"0. Prompt 我们每一次访问大模型的输入为一个 Prompt，而大模型给我们的返回结果则被称为 Completion。 Prompt 是输入指令、控制模型行为的主要方式","title":"大模型微调基础概念"},{"content":"白光 白光定义为：由不同颜色的混合光，包含了光谱中所有颜色（波长），如紫外光、可见光和红外光等，所以其明度最高，称之为“白光”。灰光和彩光也是在这个范围内讨论的，只不过范围在近红外区域。\n“灰光”和“彩光”常用于光通信 光纤通信有三个通信窗口，分别是：850nm、1310nm和1550nm。\n灰光的波长是在某个范围内波动的，没有特定的标准波长，只看这个波长是否在低损耗窗口内波动，波长的波动范围一般在±40nm左右。\n灰光主要由ITU-T G.957、ITU-T G.691、ITU-T G.959.1和IEEE 802.3定义。\nG.957定义了速率STM-1、STM-4、STM-16等灰光接口； G.691定义了STM-64灰光接口，这里的G.691是对G.957的扩充； G.959.1定义了客户侧信号2.5G、10G、25G、40G以及PAM4 50G等域间灰光接口的规范； IEEE 802.3定义了各种以太网速率的接口。 彩光在某个中心波长附近很小的范围内波动。彩光本质上是单色光，是单一波长的光，在光谱是一条可以看成是一条巨脉冲。\n光通信使用的波长范围为850 ~ 1650 nm，这里光模块发的不管“彩光”还是“灰光”，都是人眼看不见的。\n“灰光”和“彩光”对比 在成本上，一般来说彩光模块要比灰光模块成本高，因为灰光只要激光器发光在低损耗窗口就可以了。而彩光模块需要约束激光器发光的中心频率在一定的精度内（最大中心频率偏差），还要控制温度（TEC）以保证其波长的稳定性。\n我们通常会在距离比较短光纤资源丰富的场景中选用灰光模块。在光纤距离远光纤不足的地方中选用彩光模块。\n类别 灰光模块 彩光模块 波长 灰光中心波长范围：约为30nm 彩光中心波长范围：+-6.5nm /CWDM ，0.04nm/DWDM 100GHZ 应用 多用于客户侧 多用于线路侧 特点 功耗小、适用近距离传输 带宽容量大、节约光纤资源 遵循标准 ITU-T G.957；ITU-T G.959.1；IEEE 802.3 ITU-T G.694.1（DWDM）；ITU-T G694.2（CWDM） 小结 中心频率： 是否有中心频率 白光 无 不同颜色的混合光，包含了光谱中所有颜色 灰光 无 在某个范围内波动的 彩光 有 某个中心波长附近很小的范围内波动 光谱范围：白光\u0026gt;灰光\u0026gt;彩光 参考：\n[1] 光与技术：灰光与彩光有什么不同？\n[2] 一文讲解灰光模块和彩光模块 - 知乎 (zhihu.com)\n","permalink":"https://luozijian1223.github.io/posts/tech/%E4%BB%80%E4%B9%88%E6%98%AF%E7%99%BD%E5%85%89%E7%81%B0%E5%85%89%E5%92%8C%E5%BD%A9%E5%85%89/","summary":"白光 白光定义为：由不同颜色的混合光，包含了光谱中所有颜色（波长），如紫外光、可见光和红外光等，所以其明度最高，称之为“白光”。灰光和彩光也是","title":"什么是白光、灰光和彩光？"},{"content":"这是我的第一篇博文！\n","permalink":"https://luozijian1223.github.io/posts/life/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E6%96%87/","summary":"这是我的第一篇博文！","title":"第一篇博文"},{"content":"class Me:\rdef __init__(self):\rself.name = \u0026#34;Hawoo\u0026#34;\rself.english_name = \u0026#34;Elbert\u0026#34;\rself.born_year = 2002\rself.MBTI = \u0026#34;INTP\u0026#34;\rself.location = \u0026#34;Nanjing, Jiangsu, CN\u0026#34;\rself.undergrad_school = \u0026#34;Nanjing University of Post and Technology\u0026#34; Hi! Welcome to my Blog.\nMy name is Hawoo~ 我叫哈沃，英文名叫Elbert，今年22岁，就读于南京邮电大学光电信息科学与工程专业。\n","permalink":"https://luozijian1223.github.io/about/","summary":"class Me: def __init__(self): self.name = \u0026#34;Hawoo\u0026#34; self.english_name = \u0026#34;Elbert\u0026#34; self.born_year = 2002 self.MBTI = \u0026#34;INTP\u0026#34; self.location = \u0026#34;Nanjing, Jiangsu, CN\u0026#34; self.undergrad_school = \u0026#34;Nanjing University of Post and Technology\u0026#34; Hi! Welcome to my Blog. My name is Hawoo~ 我叫哈沃，英文名叫Elbert，今年22岁，就读于南京邮电大学光","title":"🙋🏻‍♂️关于我"}]