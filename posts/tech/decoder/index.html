<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Decoder | Luo's Blog</title>
<meta name=keywords content="Transformer,AI"><meta name=description content="![[Pasted image 20250218162344.png]] 填充（padding）和掩码（masking）机制 在解码器（decoder）中使用掩码（mask）的目的是确保当前位置的输出仅仅依赖于"><meta name=author content="Luo"><link rel=canonical href=https://luozijian1223.github.io/posts/tech/decoder/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://luozijian1223.github.io/images/Q.jpg><link rel=icon type=image/png sizes=16x16 href=https://luozijian1223.github.io/images/Q.jpg><link rel=icon type=image/png sizes=32x32 href=https://luozijian1223.github.io/images/Q.jpg><link rel=apple-touch-icon href=https://luozijian1223.github.io/images/Q.jpg><link rel=mask-icon href=https://luozijian1223.github.io/images/Q.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://luozijian1223.github.io/posts/tech/decoder/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Decoder"><meta property="og:description" content="![[Pasted image 20250218162344.png]] 填充（padding）和掩码（masking）机制 在解码器（decoder）中使用掩码（mask）的目的是确保当前位置的输出仅仅依赖于"><meta property="og:type" content="article"><meta property="og:url" content="https://luozijian1223.github.io/posts/tech/decoder/"><meta property="og:image" content="https://luozijian1223.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-17T15:39:41+08:00"><meta property="article:modified_time" content="2025-02-17T15:39:41+08:00"><meta property="og:site_name" content="Luo's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://luozijian1223.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Decoder"><meta name=twitter:description content="![[Pasted image 20250218162344.png]] 填充（padding）和掩码（masking）机制 在解码器（decoder）中使用掩码（mask）的目的是确保当前位置的输出仅仅依赖于"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://luozijian1223.github.io/posts/"},{"@type":"ListItem","position":2,"name":"👨🏻‍💻技术","item":"https://luozijian1223.github.io/posts/tech/"},{"@type":"ListItem","position":3,"name":"Decoder","item":"https://luozijian1223.github.io/posts/tech/decoder/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Decoder","name":"Decoder","description":"![[Pasted image 20250218162344.png]] 填充（padding）和掩码（masking）机制 在解码器（decoder）中使用掩码（mask）的目的是确保当前位置的输出仅仅依赖于","keywords":["Transformer","AI"],"articleBody":"![[Pasted image 20250218162344.png]]\n填充（padding）和掩码（masking）机制 在解码器（decoder）中使用掩码（mask）的目的是确保当前位置的输出仅仅依赖于其之前的位置，而不是未来的位置。\n掩码（mask）的作用 1. 因果关系（Causality） 在序列生成中，每个输出仅仅应当依赖于之前的输出。例如，生成句子的第四个单词时，我们只能考虑前三个单词，而不能考虑第五个单词。加入掩码确保模型在训练时保持这种因果关系。\n2. 训练与推理的一致性 在推理（即实际使用）时，我们通常采用自回归方式生成序列：一次生成一个标记，然后将其作为新输入传递回模型。使用掩码确保模型在训练和推理时的行为是一致的。\n3. 避免信息泄露 如果不使用掩码，模型可能会在训练期间“窥视”未来的标记，并过度依赖这些信息。这会导致在推理时性能下降，因为在实际应用中这些未来的标记是不可用的。\n在实际操作中，掩码通常是一个上三角形状的矩阵，其中上三角（包括对角线）的部分设为0，其余部分设为负无穷（或非常大的负数）。在应用 softmax 函数计算注意力权重之前，这个掩码会被加到注意力分数上，这样上三角部分的分数在 softmax 后基本上就会变成0，从而实现了掩蔽效果。\n举例 “我喜欢猫猫” “我喜欢打羽毛球” 步骤 1：Tokenization 我们将每个句子分词，得到以下token序列：\n1. “我” “喜” “欢” “猫” “猫” 2. “我” “喜” “欢” “打” “羽” “毛” “球” 步骤 2：Padding 由于训练集中最长的句子长度为10个tokens，我们需要对这两个句子进行填充，使它们的长度都达到10个tokens。通常使用一个特殊的填充token（例如\"\"）来实现这一点：\n1. “我” “喜” “欢” “猫” “猫” “” “” “” “” “” 2. “我” “喜” “欢” “打” “羽” “毛” “球” “” “” “” 步骤 3：Masking 现在，模型需要知道哪些是真正的token，哪些是填充的token。这是通过创建一个掩码矩阵来实现的，掩码矩阵与输入序列的长度相同。在这个掩码矩阵中，真实token的位置用0表示，填充token的位置用1表示：\n1. 掩码：[0, 0, 0, 0, 0, 1, 1, 1, 1, 1] 2. 掩码：[0, 0, 0, 0, 0, 0, 0, 1, 1, 1] 步骤 4：应用掩码 在模型的注意力机制中，特别是在计算softmax之前，我们会将这些掩码应用到模型中。具体来说，我们会在掩码为1的位置上加上一个非常大的负数（例如负无穷），这样在经过softmax操作后，这些位置的值将接近0，从而不会对全局概率预测产生影响。\n通过这种方式，模型在计算注意力权重时，会忽略填充token，确保只关注实际的内容token。这样，模型就能够正确地学习序列数据，而不会被填充token所干扰。\nMask-Multi-Head-Attention Mask 的目的是防止 Decoder “seeing the future”，就像防止考生偷看考试答案一样。\n[1](Pasted image 20250218182106.png)\nLinear 和 Softmax 来生成输出概率 在解码器的最终阶段，其输出将通过一个线性层进行转换，该层的作用是将解码器的输出映射到与词汇表大小相匹配的维度。随后，应用softmax函数将这些数值转换成概率分布，其中每个词汇对应一个概率值。在这一过程中，概率最高的词汇被选为预测的下一个词。\n训练过程 模型没有收敛得很好时，Decoder预测产生的词很可能不是我们想要的。这个时候如果再把错误的数据再输给Decoder，就会越跑越偏。这个时候怎么办？\n(1）在训练过程中可以使用 “teacher forcing”。因为我们知道应该预测的word是什么，那么可以给Decoder喂一个正确的结果作为输入。\n(2）除了选择最高概率的词 (greedy search)，还可以选择是比如 “Beam Search”，保留top-K个预测的word。 Beam Search 方法不再是只得到一个输出放到下一步去训练了，我们可以设定一个值，拿多个值放到下一步去训练，这条路径的概率等于每一步输出的概率的乘积。\n训练和损失函数 在训练过程中，我们使用损失函数（例如交叉熵损失）将生成的输出概率分布与目标序列进行比较。概率分布给出了每个单词出现在该位置的概率。\nDecoder在不同阶段的信息传递机制 训练阶段 自由运行（Free Running）模式 在训练阶段，解码器使用上一个时间单元的预测值作为下一个时间单元的输入。这种方法的缺点是，如果模型在某一步预测错误，那么这个错误会随着时间单元的推进而累积，导致预测结果越来越偏离正确轨迹，从而增加了训练的难度。 教师强制（Teacher Forcing）模式 Teacher Forcing模式在训练时使用真实的标签（label）作为输入，而不是模型的预测值。这样，即使模型在前一个时间单元预测错误，它仍然可以在下一个时间单元接收到正确的信息，从而更容易地学习和收敛。但是，这种方法的局限性在于，它在训练和预测阶段存在不一致性，因为在实际预测时，我们无法获得真实的标签。 结合这两种方法的优点，在训练阶段采用了一种称为计划采样（scheduled sampling）的策略。\n核心思想：在训练过程中动态地结合真实标签和模型的预测。 具体做法：设定一个概率值p，以p的概率，使用上一个时间单元的预测值作为输入，以1-p的概率，使用真实的标签作为输入。这样，解码器在训练时既能得到真实标签的适当指导，又能逐渐学会依赖自己的预测，从而提高了模型在预测阶段的稳健性和一致性。\n预测阶段 穷举法 贪心搜索 束搜索 在预测阶段，解码器直接将前一个时间单元的输出（即预测值）作为输入传递给下一个时间单元。 Decoder的输入选择：\n- 穷举法 softmax对所有候选词预测概率，然后使用所有的排列组合找出输出条件概率最大的序列，作为decoder的输出，可以保证全局最优，但是计算复杂度非常高\n- 贪心搜索： 使用softmax对所有候选词预测概率，选概率最大的词作为当前单元的输出，**保证每一步输出最优解，但满足局部最优策略期望未必产生全局最优解\n- 束搜索（Beam search）： 在贪心搜索上做了个优化，设置一个束宽beam size，然后softmax预测出来的候选词概率选取概率最大的beam size个词输入下一个单元，比如下面这个例子，beam size设置为2，每个时间单元会从候选词中选择概率值最大的两个词，然后输入到下一个时间单元，在计算条件概率，然后再去最大的前两个，以此类推\n模型的训练与评估 训练Transformer模型涉及优化其参数以最小化损失函数，通常使用梯度下降和反向传播。一旦训练完成，就会使用各种指标评估模型的性能，以评估其解决目标任务的有效性。\n梯度下降 (Gradient Descent)是一种迭代优化算法，用于最小化损失函数。 它通过沿着负梯度方向逐步更新参数，寻找最优解。\n反向传播 (Backpropagation)利用 链式法则，将梯度从输出层反向传播到输入层，计算出所有参数的梯度。\n训练神经网络的完整流程\n随机初始化参数 –\u003e 前向传播 –\u003e 反向传播 –\u003e 参数更新 –\u003e 重复步骤 2-4，直到模型收敛或达到预设的训练轮数\n训练过程\n(1) 梯度下降和反向传播：\n在训练期间，将输入序列输入模型，并生成输出序列。 将模型的预测与地面真相进行比较，涉及使用损失函数（例如交叉熵损失）来衡量预测值与实际值之间的差异。 梯度下降用于更新模型的参数，使损失最小化的方向。 优化器根据这些梯度调整参数，迭代更新它们以提高模型性能。 (2) 学习率调度：\n可以应用学习率调度技术来动态调整训练期间的学习率。 常见策略包括热身计划，其中学习率从低开始逐渐增加，以及衰减计划，其中学习率随时间降低。 梯度下降和反向传播 梯度下降是一种优化算法，用于最小化一个损失函数 (Loss Function)。 损失函数衡量了模型预测结果与真实结果之间的差距。 梯度下降是一种 迭代优化算法，用于寻找 损失函数的最小值。\n一些名词：\n损失函数 (Loss Function): 用 𝐽(𝜃)表示，其中 𝜃 代表模型的参数 (例如神经网络的权重和偏置)。 梯度 (Gradient): 用 ∇𝐽(𝜃) 表示，它是一个向量，指向 损失函数 𝐽(𝜃) 在参数空间中变化最快的方向 (也就是 最陡峭的上升方向)。 负梯度 (-∇J(θ)): 负梯度方向 就是 损失函数 𝐽(𝜃) 下降最快的方向，也就是我们要 “下山” 的方向。 学习率 (Learning Rate, α): 一个 超参数，控制每次迭代 参数更新的步长。 学习率过大可能导致震荡或无法收敛，学习率过小则收敛速度过慢。 参数更新规则: 在每次迭代中，我们根据 负梯度方向 和 学习率 更新参数 𝜃\nθ_(new) = θ_(old) - α * ∇J(θ_(old)) 梯度下降过程：\n初始化：随机化模型参数𝜃。 计算梯计损失函数 𝐽(𝜃) 关于参数𝜃的梯度 ∇𝐽(𝜃) 。这通常需要用到 反向传播算法 更新参数: 根据学习率α 和梯度 ∇𝐽(𝜃)，使用更新规则𝜃=𝜃−𝛼∗∇𝐽(𝜃) 更新参数。 判断是否满足停止条件: 检查是否满足停止条件 (例如梯度是否足够小，损失函数值是否收敛)。 如果满足，停止迭代，当前参数 𝜃 即为最优解 (或近似最优解)。 如果不满足，返回步骤 2，继续迭代。 模型的评估指标 BLEU（Bilingual Evaluation Understudy）：适用于机器翻译和文本生成任务，衡量生成文本与参考文本的相似度。 ROUGE（Recall-Oriented Understudy for Gisting Evaluation）：常用于文本摘要，评估生成摘要与参考摘要的重叠部分。 METEOR（Metric for Evaluation of Translation with Explicit ORdering of Renderings）：结合精确率和召回率，用于评估翻译质量。 Perplexity 困惑度：用于语言模型，衡量模型预测测试数据的能力，数值越低越好。 ","wordCount":"3774","inLanguage":"en","image":"https://luozijian1223.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-02-17T15:39:41+08:00","dateModified":"2025-02-17T15:39:41+08:00","author":[{"@type":"Person","name":"Luo"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://luozijian1223.github.io/posts/tech/decoder/"},"publisher":{"@type":"Organization","name":"Luo's Blog","logo":{"@type":"ImageObject","url":"https://luozijian1223.github.io/images/Q.jpg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://luozijian1223.github.io/ accesskey=h title="Luo's Blog (Alt + H)">Luo's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://luozijian1223.github.io/search title="🔍搜索 (Alt + /)" accesskey=/><span>🔍搜索</span></a></li><li><a href=https://luozijian1223.github.io/ title=🏠主页><span>🏠主页</span></a></li><li><a href=https://luozijian1223.github.io/posts title=📚文章><span>📚文章</span></a></li><li><a href=https://luozijian1223.github.io/projects title=⚙️项目><span>⚙️项目</span></a></li><li><a href=https://luozijian1223.github.io/archives title=⏱时间轴><span>⏱时间轴</span></a></li><li><a href=https://luozijian1223.github.io/tags title=🔖标签><span>🔖标签</span></a></li><li><a href=https://luozijian1223.github.io/about title=🙋🏻‍♂️关于><span>🙋🏻‍♂️关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://luozijian1223.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://luozijian1223.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://luozijian1223.github.io/posts/tech/>👨🏻‍💻技术</a></div><h1 class="post-title entry-hint-parent">Decoder</h1><div class=post-meta><span title='2025-02-17 15:39:41 +0800 CST'>2025-02-17</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;3774 words&nbsp;·&nbsp;Luo</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#填充padding和掩码masking机制> 填充（padding）和掩码（masking）机制</a></li><li><a href=#mask-multi-head-attention>Mask-Multi-Head-Attention</a></li><li><a href=#linear-和-softmax-来生成输出概率>Linear 和 Softmax 来生成输出概率</a></li><li><a href=#训练过程>训练过程</a></li><li><a href=#训练和损失函数>训练和损失函数</a></li></ul></li><li><a href=#decoder在不同阶段的信息传递机制>Decoder在不同阶段的信息传递机制</a><ul><li><a href=#训练阶段>训练阶段</a></li><li><a href=#预测阶段>预测阶段</a></li></ul></li><li><a href=#模型的训练与评估>模型的训练与评估</a><ul><li><a href=#梯度下降和反向传播>梯度下降和反向传播</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>![[Pasted image 20250218162344.png]]</p><h3 id=填充padding和掩码masking机制> 填充（padding）和掩码（masking）机制<a hidden class=anchor aria-hidden=true href=#填充padding和掩码masking机制>#</a></h3><p>在解码器（decoder）中使用掩码（mask）的<strong>目的是</strong>确保当前位置的输出仅仅依赖于其之前的位置，而不是未来的位置。</p><h4 id=掩码mask的作用>掩码（mask）的作用<a hidden class=anchor aria-hidden=true href=#掩码mask的作用>#</a></h4><p><strong>1. 因果关系（Causality）</strong>
在序列生成中，每个输出仅仅应当依赖于之前的输出。例如，生成句子的第四个单词时，我们只能考虑前三个单词，而不能考虑第五个单词。加入掩码确保模型在训练时保持这种因果关系。</p><p><strong>2. 训练与推理的一致性</strong>
在推理（即实际使用）时，我们通常采用自回归方式生成序列：一次生成一个标记，然后将其作为新输入传递回模型。使用掩码确保模型在训练和推理时的行为是一致的。</p><p><strong>3. 避免信息泄露</strong>
如果不使用掩码，模型可能会在训练期间“窥视”未来的标记，并过度依赖这些信息。这会导致在推理时性能下降，因为在实际应用中这些未来的标记是不可用的。</p><p><strong>在实际操作中</strong>，掩码通常是一个上三角形状的矩阵，其中上三角（包括对角线）的部分设为0，其余部分设为负无穷（或非常大的负数）。在应用 softmax 函数计算注意力权重之前，这个掩码会被加到注意力分数上，这样上三角部分的分数在 softmax 后基本上就会变成0，从而实现了掩蔽效果。</p><h4 id=举例>举例<a hidden class=anchor aria-hidden=true href=#举例>#</a></h4><ul><li>“我喜欢猫猫”</li><li>“我喜欢打羽毛球”</li></ul><p><strong>步骤 1：Tokenization</strong> 我们将每个句子分词，得到以下token序列：</p><pre tabindex=0><code>1. “我” “喜” “欢” “猫” “猫”
2. “我” “喜” “欢” “打” “羽” “毛” “球”
</code></pre><p><strong>步骤 2：Padding</strong> 由于训练集中最长的句子长度为10个tokens，我们需要对这两个句子进行填充，使它们的长度都达到10个tokens。通常使用一个特殊的填充token（例如""）来实现这一点：</p><pre tabindex=0><code>1. “我” “喜” “欢” “猫” “猫” “” “” “” “” “”
2. “我” “喜” “欢” “打” “羽” “毛” “球” “” “” “”
</code></pre><p><strong>步骤 3：Masking</strong> 现在，模型需要知道哪些是真正的token，哪些是填充的token。这是通过创建一个掩码矩阵来实现的，掩码矩阵与输入序列的长度相同。在这个掩码矩阵中，真实token的位置用0表示，填充token的位置用1表示：</p><pre tabindex=0><code>1. 掩码：[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
2. 掩码：[0, 0, 0, 0, 0, 0, 0, 1, 1, 1]
</code></pre><p><strong>步骤 4：应用掩码</strong> 在模型的注意力机制中，特别是在计算softmax之前，我们会将这些掩码应用到模型中。具体来说，我们会在掩码为1的位置上加上一个非常大的负数（例如负无穷），这样在经过softmax操作后，这些位置的值将接近0，从而不会对全局概率预测产生影响。</p><p>通过这种方式，模型在计算注意力权重时，会忽略填充token，确保只关注实际的内容token。这样，模型就能够正确地学习序列数据，而不会被填充token所干扰。</p><h3 id=mask-multi-head-attention>Mask-Multi-Head-Attention<a hidden class=anchor aria-hidden=true href=#mask-multi-head-attention>#</a></h3><p>Mask 的目的是防止 Decoder “seeing the future”，就像防止考生偷看考试答案一样。</p><p>[1](Pasted image 20250218182106.png)</p><h3 id=linear-和-softmax-来生成输出概率>Linear 和 Softmax 来生成输出概率<a hidden class=anchor aria-hidden=true href=#linear-和-softmax-来生成输出概率>#</a></h3><p>在解码器的最终阶段，其输出将通过一个线性层进行转换，<strong>该层的作用是将解码器的输出映射到与词汇表大小相匹配的维度</strong>。随后，应用softmax函数将这些数值转换成概率分布，其中每个词汇对应一个概率值。在这一过程中，<strong>概率最高的词汇被选为预测的下一个词</strong>。</p><h3 id=训练过程>训练过程<a hidden class=anchor aria-hidden=true href=#训练过程>#</a></h3><p>模型没有收敛得很好时，Decoder预测产生的词很可能不是我们想要的。这个时候如果再把错误的数据再输给Decoder，就会越跑越偏。这个时候怎么办？</p><p>(1）在训练过程中可以使用 “teacher forcing”。因为我们知道应该预测的word是什么，那么可以给Decoder喂一个正确的结果作为输入。</p><p>(2）除了选择最高概率的词 (greedy search)，还可以选择是比如 “Beam Search”，保留top-K个预测的word。 Beam Search 方法不再是只得到一个输出放到下一步去训练了，我们可以设定一个值，拿多个值放到下一步去训练，这条路径的概率等于每一步输出的概率的乘积。</p><h3 id=训练和损失函数>训练和损失函数<a hidden class=anchor aria-hidden=true href=#训练和损失函数>#</a></h3><p>在训练过程中，我们使用损失函数（例如交叉熵损失）将生成的输出概率分布与目标序列进行比较。概率分布给出了每个单词出现在该位置的概率。</p><h2 id=decoder在不同阶段的信息传递机制>Decoder在不同阶段的信息传递机制<a hidden class=anchor aria-hidden=true href=#decoder在不同阶段的信息传递机制>#</a></h2><h3 id=训练阶段>训练阶段<a hidden class=anchor aria-hidden=true href=#训练阶段>#</a></h3><blockquote><ul><li>自由运行（Free Running）模式
在训练阶段，解码器使用上一个时间单元的预测值作为下一个时间单元的输入。这种方法的<strong>缺点是</strong>，如果模型在某一步预测错误，那么这个错误会随着时间单元的推进而<strong>累积</strong>，导致预测结果越来越偏离正确轨迹，从而增加了训练的难度。</li><li>教师强制（Teacher Forcing）模式
Teacher Forcing模式在训练时<strong>使用真实的标签（label）作为输入</strong>，而不是模型的预测值。这样，即使模型在前一个时间单元预测错误，它仍然可以在下一个时间单元接收到正确的信息，从而<strong>更容易地学习和收敛</strong>。但是，这种方法的<strong>局限性</strong>在于，它在训练和预测阶段存在不一致性，因为在实际预测时，我们无法获得真实的标签。</li></ul></blockquote><p>结合这两种方法的优点，在训练阶段采用了一种称为计划采样（scheduled sampling）的策略。</p><p>核心思想：<strong>在训练过程中动态地结合真实标签和模型的预测</strong>。
具体做法：设定一个概率值p，以p的概率，使用上一个时间单元的预测值作为输入，以1-p的概率，使用真实的标签作为输入。这样，解码器在训练时既能得到真实标签的适当指导，又能逐渐学会依赖自己的预测，从而提高了模型在预测阶段的稳健性和一致性。</p><h3 id=预测阶段>预测阶段<a hidden class=anchor aria-hidden=true href=#预测阶段>#</a></h3><ul><li>穷举法</li><li>贪心搜索</li><li>束搜索</li></ul><p>在预测阶段，解码器直接将前一个时间单元的输出（即预测值）作为输入传递给下一个时间单元。 Decoder的输入选择：</p><p><strong>- 穷举法</strong>
softmax对所有候选词预测概率，然后使用所有的排列组合找出输出条件概率最大的序列，作为decoder的输出，可以保证全局最优，但是计算复杂度非常高</p><p><strong>- 贪心搜索：</strong>
使用softmax对所有候选词预测概率，选概率最大的词作为当前单元的输出，**保证每一步输出最优解，但满足局部最优策略期望未必产生全局最优解</p><p><strong>- 束搜索（Beam search）：</strong>
在贪心搜索上做了个优化，设置一个束宽beam size，然后softmax预测出来的候选词概率选取概率最大的beam size个词输入下一个单元，比如下面这个例子，beam size设置为2，每个时间单元会从候选词中选择概率值最大的两个词，然后输入到下一个时间单元，在计算条件概率，然后再去最大的前两个，以此类推</p><h2 id=模型的训练与评估>模型的训练与评估<a hidden class=anchor aria-hidden=true href=#模型的训练与评估>#</a></h2><p>训练Transformer模型涉及优化其参数以最小化损失函数，通常使用<strong>梯度下降和反向传播</strong>。一旦训练完成，就会使用各种指标评估模型的性能，以评估其解决目标任务的有效性。</p><p>梯度下降 (Gradient Descent)是一种迭代优化算法，用于<strong>最小化损失函数</strong>。 它通过沿着<strong>负梯度方向</strong>逐步更新参数，寻找最优解。</p><p>反向传播 (Backpropagation)利用 <strong>链式法则</strong>，将梯度从输出层反向传播到输入层，计算出所有参数的梯度。</p><blockquote><p>训练神经网络的完整流程</p><p>随机初始化参数 &ndash;> 前向传播 &ndash;> 反向传播 &ndash;> 参数更新 &ndash;> 重复步骤 2-4，直到模型收敛或达到预设的训练轮数</p></blockquote><p><strong>训练过程</strong></p><p>(1) 梯度下降和反向传播：</p><ul><li>在训练期间，将输入序列输入模型，并生成输出序列。</li><li>将模型的预测与地面真相进行比较，涉及使用损失函数（例如<strong>交叉熵损失</strong>）来衡量预测值与实际值之间的差异。</li><li>梯度下降用于更新模型的参数，使损失最小化的方向。</li><li>优化器根据这些梯度调整参数，迭代更新它们以提高模型性能。</li></ul><p>(2) 学习率调度：</p><ul><li>可以应用学习率调度技术来动态调整训练期间的学习率。</li><li>常见策略包括热身计划，其中学习率从低开始逐渐增加，以及衰减计划，其中学习率随时间降低。</li></ul><h3 id=梯度下降和反向传播>梯度下降和反向传播<a hidden class=anchor aria-hidden=true href=#梯度下降和反向传播>#</a></h3><blockquote><p> 梯度下降是一种优化算法，用于<strong>最小化一个损失函数 (Loss Function)</strong>。 损失函数衡量了模型预测结果与真实结果之间的差距。 梯度下降是一种 <strong>迭代优化算法</strong>，用于寻找 <strong>损失函数的最小值</strong>。</p></blockquote><p>一些名词：</p><ul><li><strong>损失函数 (Loss Function):</strong> 用 𝐽(𝜃)表示，其中  <code>𝜃</code> 代表模型的参数 (例如神经网络的权重和偏置)。</li><li><strong>梯度 (Gradient):</strong> 用  ∇𝐽(𝜃) 表示，它是一个向量，指向 <strong>损失函数  𝐽(𝜃) 在参数空间中变化最快的方向</strong> (也就是 <strong>最陡峭的上升方向</strong>)。</li><li><strong>负梯度 (-∇J(θ)):</strong> <strong>负梯度方向</strong> 就是 <strong>损失函数</strong> <strong>𝐽(𝜃)</strong> <strong>下降最快的方向</strong>，也就是我们要 &ldquo;下山&rdquo; 的方向。</li><li><strong>学习率 (Learning Rate, α):</strong> 一个 <strong>超参数</strong>，控制每次迭代 <strong>参数更新的步长</strong>。 学习率过大可能导致震荡或无法收敛，学习率过小则收敛速度过慢。</li></ul><p><strong>参数更新规则:</strong> 在每次迭代中，我们根据 <strong>负梯度方向</strong> 和 <strong>学习率</strong> 更新参数 𝜃</p><pre tabindex=0><code>θ_(new) = θ_(old) - α * ∇J(θ_(old))
</code></pre><p>梯度下降过程：</p><ol><li>初始化：随机化模型参数𝜃。</li><li>计算梯计损失函数 𝐽(𝜃) 关于参数𝜃的梯度 ∇𝐽(𝜃) 。这通常需要用到 <strong>反向传播算法</strong></li><li><strong>更新参数:</strong> 根据学习率α 和梯度 ∇𝐽(𝜃)，使用更新规则𝜃=𝜃−𝛼∗∇𝐽(𝜃) 更新参数。</li><li><strong>判断是否满足停止条件:</strong> 检查是否满足停止条件 (例如梯度是否足够小，损失函数值是否收敛)。<ul><li><strong>如果满足，停止迭代，当前参数</strong> <strong><code>𝜃</code></strong> <strong>即为最优解 (或近似最优解)。</strong></li><li><strong>如果不满足，返回步骤 2，继续迭代。</strong></li></ul></li></ol><h4 id=模型的评估指标>模型的评估指标<a hidden class=anchor aria-hidden=true href=#模型的评估指标>#</a></h4><ul><li><strong>BLEU（Bilingual Evaluation Understudy）</strong>：适用于机器翻译和文本生成任务，衡量生成文本与参考文本的相似度。</li><li><strong>ROUGE（Recall-Oriented Understudy for Gisting Evaluation）</strong>：常用于文本摘要，评估生成摘要与参考摘要的重叠部分。</li><li><strong>METEOR（Metric for Evaluation of Translation with Explicit ORdering of Renderings）</strong>：结合精确率和召回率，用于评估翻译质量。</li><li><strong>Perplexity 困惑度</strong>：用于语言模型，衡量模型预测测试数据的能力，数值越低越好。</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://luozijian1223.github.io/tags/transformer/>Transformer</a></li><li><a href=https://luozijian1223.github.io/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://luozijian1223.github.io/posts/read/%E6%97%A0%E4%BA%BA%E7%94%9F%E8%BF%98/><span class=title>« Prev</span><br><span>《无人生还》：孤岛上的罪恶、审判与人性挣扎</span>
</a><a class=next href=https://luozijian1223.github.io/posts/tech/%E6%B5%8F%E8%A7%88%E5%99%A8/><span class=title>Next »</span><br><span>浏览器</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://luozijian1223.github.io/>Luo's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>